<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Performance of Symbolic &amp; Connectionist AI in Image Recognition" /><meta name="author" content="Max van Dijck" /><meta property="og:locale" content="en_US" /><meta name="description" content="Neural network approaches in the field of artificial intelligence (AI) have accelerated in popularity over the past decade in the academic field. This surge has arguably left behind import concerns in intelligent machines such as interpretability and robustness. This raises questions around the trade-off of performance and aforementioned concerns when choosing between symbolic and connectionist approaches to solving AI problems. This study explores three different models, Deep learning, K-Nearest Neighbour and Support Vector Machines across two separate image datasets. The intent of this is to evaluate the performance of image recognition approaches across methods of different interpretability when approaching classification challenges of high-dimensional data. The study found that symbolic approaches can compete well within the range of Deep Learning capabilities on datasets with a large volume of training data and lower dimensionality. However, when faced with little training data a transfer learning approach significantly outperformed the other methods. A downside to K-NN was exhibited in its inference time where it took significantly longer than the other methods to label the test data." /><meta property="og:description" content="Neural network approaches in the field of artificial intelligence (AI) have accelerated in popularity over the past decade in the academic field. This surge has arguably left behind import concerns in intelligent machines such as interpretability and robustness. This raises questions around the trade-off of performance and aforementioned concerns when choosing between symbolic and connectionist approaches to solving AI problems. This study explores three different models, Deep learning, K-Nearest Neighbour and Support Vector Machines across two separate image datasets. The intent of this is to evaluate the performance of image recognition approaches across methods of different interpretability when approaching classification challenges of high-dimensional data. The study found that symbolic approaches can compete well within the range of Deep Learning capabilities on datasets with a large volume of training data and lower dimensionality. However, when faced with little training data a transfer learning approach significantly outperformed the other methods. A downside to K-NN was exhibited in its inference time where it took significantly longer than the other methods to label the test data." /><link rel="canonical" href="/posts/Performance-of-Symbolic-&-Connectionist-AI-in-Image-Recognition/" /><meta property="og:url" content="/posts/Performance-of-Symbolic-&-Connectionist-AI-in-Image-Recognition/" /><meta property="og:site_name" content="Max van Dijck" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-06-17T08:55:00+12:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Performance of Symbolic &amp; Connectionist AI in Image Recognition" /><meta name="twitter:site" content="@Max_vanDijck" /><meta name="twitter:creator" content="@Max van Dijck" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Max van Dijck"},"dateModified":"2021-06-17T08:55:00+12:00","datePublished":"2021-06-17T08:55:00+12:00","description":"Neural network approaches in the field of artificial intelligence (AI) have accelerated in popularity over the past decade in the academic field. This surge has arguably left behind import concerns in intelligent machines such as interpretability and robustness. This raises questions around the trade-off of performance and aforementioned concerns when choosing between symbolic and connectionist approaches to solving AI problems. This study explores three different models, Deep learning, K-Nearest Neighbour and Support Vector Machines across two separate image datasets. The intent of this is to evaluate the performance of image recognition approaches across methods of different interpretability when approaching classification challenges of high-dimensional data. The study found that symbolic approaches can compete well within the range of Deep Learning capabilities on datasets with a large volume of training data and lower dimensionality. However, when faced with little training data a transfer learning approach significantly outperformed the other methods. A downside to K-NN was exhibited in its inference time where it took significantly longer than the other methods to label the test data.","headline":"Performance of Symbolic &amp; Connectionist AI in Image Recognition","mainEntityOfPage":{"@type":"WebPage","@id":"/posts/Performance-of-Symbolic-&-Connectionist-AI-in-Image-Recognition/"},"url":"/posts/Performance-of-Symbolic-&-Connectionist-AI-in-Image-Recognition/"}</script><title>Performance of Symbolic & Connectionist AI in Image Recognition | Max van Dijck</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/portrait.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Max van Dijck</a></div><div class="site-subtitle font-italic">Aspiring Machine Learning Practitioner</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/MaxVanDijck" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/Max_vanDijck" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['vandijcksm','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Performance of Symbolic & Connectionist AI in Image Recognition</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Performance of Symbolic & Connectionist AI in Image Recognition</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Max van Dijck </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Jun 17, 2021, 8:55 AM +1200" prep="on" > Jun 17, 2021 <i class="unloaded">2021-06-17T08:55:00+12:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2797 words">15 min</span></div></div><div class="post-content"><p>Neural network approaches in the field of artificial intelligence (AI) have accelerated in popularity over the past decade in the academic field. This surge has arguably left behind import concerns in intelligent machines such as interpretability and robustness. This raises questions around the trade-off of performance and aforementioned concerns when choosing between symbolic and connectionist approaches to solving AI problems. This study explores three different models, Deep learning, K-Nearest Neighbour and Support Vector Machines across two separate image datasets. The intent of this is to evaluate the performance of image recognition approaches across methods of different interpretability when approaching classification challenges of high-dimensional data. The study found that symbolic approaches can compete well within the range of Deep Learning capabilities on datasets with a large volume of training data and lower dimensionality. However, when faced with little training data a transfer learning approach significantly outperformed the other methods. A downside to K-NN was exhibited in its inference time where it took significantly longer than the other methods to label the test data.</p><h2 id="introduction">Introduction</h2><p>The purpose of this research is to provide a foundation argument for why the consideration of a range of approaches to data driven problems should be a necessary step for machine learning practitioners and data scientists to take. In the first instance one may look at a machine learning prediction problem and choose the highest accuracy approach, this is fostered by platforms such as Kaggle (Kaggle Inc, 2021) and competitions such as the ImageNet Classification Challenge (Stanford Image Lab, 2020). While this encourages the development and use of State-of-the-Art techniques little is done to question the interpretability of the results. While this is not necessarily a bad thing, one may argue that the reliance upon black-box models may prove consequential for the field in the event of a paradigm shift or regulation.</p><p>This study compares multiple instances of AI approaches and aims to identify possible trajectories that should be explored in situations where AI is not merely used as a tool to produce the most accurate model but where the model may have several drawbacks regarding the impact on its environment or users. Three types of techniques, Deep Learning, Support Vector Machines and K-Nearest Neighbour will be evaluated by their performance across two image datasets of two separate complexities and sizes. This aims to highlight the ability of each model on challenging high-dimensional data and evaluate how each model’s performance drops when the number of training samples lower and the dimensionality further increases.</p><h2 id="literature-review">Literature Review</h2><p>Machine learning has become a highly popular field in recent years due to factors not limited to but including; the abundance of cheap computing, the mass generation and availability of data and the creation of open access tools and methods (Brownlee, 2020) (Dean, 2019). This growth is also commonly attributed to artificial intelligences high exposure in the media with claims that AI is either “Impossible” or “Just around the corner” (Allen, 1998) alongside its increase in usage in a wide range of disciplines including science, business, commercial settings, industry, engineering, entertainment and government (Bunz, 2019). However, rushed deployment of AI has resulted in cases where “the technology (has) not being tested thoroughly before-hand, which is leading to cases of injustice often related to machine bias” – Bunz. This is clear and harmful in multiple instances. For example, Colorado previously implemented more than 900 incorrect rules into its public benefits system causing pregnant woman to be denied Medicaid. Or in 2011 where Idaho implemented an algorithm for allocating home care and community integration funds that then dropped funds for the severely disabled community by as much as 42 percent (Institute for Healthcare Policy &amp; Innovation, 2018).</p><p>Due to the wide usage and commonality of connectionist AI in society, it becomes concerning that these algorithms are becoming increasingly complicated and uninterpretable over time (Christian Berghoff, 2020) but that these changes are driven by creating solutions to further uninterpretable problems such as Adversarial Image Attacks (Ian J. Goodfellow, 2015) and Trojaning Atta¬¬cks (Yingqui Liu, 2018). Solutions to these problems include creating Sparse Representation Networks (Yiwem Gui, 2018) and Network Ensembles (Tao, 2019) which are comprised of larger or multiple networks which further complicate the issue of interpretability and explainability. The concerns of society towards these algorithms is evident as international organisation, The United Nations, has acted to restrict unexplainable automated decision-making of which the user will be significantly affected (Brynce Goodman, 2016). The law creates a “right of explanation” where users can ask for explanations of decisions made about them. Due to this demand of algorithmic transparency certain use cases of machine learning approaches and other high-dimensionality algorithms are in a grey area when it comes to widespread use cases especially in medical and governmental use cases. Given the pressure from society and the combination of governments and organizations towards implementers of these algorithms. One must begin to look at the trade-off between creating interpretable, explainable AI with lower performance and accuracy compared to their complex unexplainable counterparts.</p><h2 id="datasets">Datasets</h2><h3 id="mnist-handwritten-digits">MNIST Handwritten Digits</h3><p>The MNIST handwritten digit dataset (Yann LeCun, n.d.) is a common benchmark model for comparing algorithms. It consists of 70,000 28x28 pixel greyscale images of the digits 0-9. For this model evaluation there will be 60,000 images for the training set and 10,000 remaining images for the test set. This dataset was chosen for its low dimensionality compared to other image datasets and it’s high volume of training and testing samples.</p><h3 id="nz-native-birds">NZ Native Birds</h3><p>The NZ Native Birds dataset is a self-collated image dataset of varying bird species. The dataset is purposefully created to be a challenging high-resolution, low training data dataset containing 10 categories with approximately 150 images per category. This high complexity is designed to challenge the different approaches of each model and provide a platform to easily analyse shortcomings of said models. The images were collected using Microsoft Azures Bing Image Search and cleaned by initially training a deep learning network and using said network to identify images with higher loss. These images with higher loss were then inspected and removed or recategorized accordingly. Dataset is available here: <a href="https://drive.google.com/file/d/1-DNgRR16OJsoJDbPxiW87Ja-SklRh-mW/view?usp=sharing"><strong>NZ Native Birds</strong></a></p><h2 id="image-recognition-models">Image Recognition Models</h2><p>The focus of this research is to compare different AI models on the aforementioned datasets. As such, a short description of each model followed by an in-depth explanation into their respective architectures used in this study will be provided.</p><h3 id="deep-learning--transfer-learning">Deep Learning &amp; Transfer Learning</h3><p>Deep learning comes from the connectionist approach of recreating the biological process the human brain uses for learning. This is achieved through creating layers of artificial neurons which are connected to previous and subsequent layers. Each neuron has its own weight and bias which is applied to the output of the previous layer, this process continues through all layers of the network until it reaches the final layer where each neurons output represents a probability of a certain outcome. This approach is reliant upon the Universal Approximation Theorem proven in 1991 by Kurt Hornik (Hornik, 1991) which states that the result of the first layer of a neural network can approximate any well-behaved function, this function can also be approximated by a network of greater depth.</p><p>There are two Deep Learning models used in this study. Both models use the same ResNet50 (residual network) architecture first published in 2015 (Kaiming He, 2016). The ResNet architecture is designed in a way to prevent vanishing and exploding gradients, this is caused by the exponential increase or decrease of gradients due to training with stochastic gradient descent. These gradients can cause the model to either become unable to learn or cause the model to get stuck in a local minimum unable to find the global minimum to approximate the target function correctly (Seshapanpu, 2020). The introduction of skip connections in the ResNet architecture greatly reduces these gradient issues and allows the training of much more complex, deeper networks. The effect of these skip layers is best illustrated in Hao Li et al.’s “Visualizing the Loss Landscape of Neural Nets” (Hao Li, 2017). Li shows that using skip connections smooths the loss function which makes training easier as it avoids many local minimums. <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/2021-06-17/training_landscape.png" alt="Training Landscape Graphs" /> <em>Fig 1. Visualised CNN training landscape (left) ResNet training landscape (right) (Hao Li, 2017)</em> The difference between the two models is that the first model will have used transfer learning and the second model will be trained completely from scratch. Transfer learning is the act of taking a pretrained model, in this case, on a large dataset such as ImageNet and adjusting the model to fit to a new dataset. This adjustment is achieved by preserving all the high-quality edge detection in the early layers using a lower learning rate and retrains the ‘deepest’ layers of the network with a higher learning rate.</p><h3 id="k-nearest-neighbour">K-Nearest Neighbour</h3><p>The K-nearest Neighbour algorithm (k-NN) was first introduced by Fix &amp; Hodges as a classification method in 1951 (Evenlyn Fix, 1951). The method works by calculating the distance between datapoints in the classification target and the training data, the classification target can then be labelled according the surrounding data’s labels taking in account a certain number of surrounding data (k = 𝑥). This algorithm has a few notable advantages. Firstly, there is no need to build a model with many parameters to tune. Secondly, k-NN is simple to implement and interpret the choices the model makes. A drawback of the k-NN algorithm arises in high-dimensional data whereas the number of dimensions and classification options increase the distances become close to equal lowering the reliability and accuracy of the classifier.</p><p>For image detection we will be taking the difference between values of each pixel in the training-set image and the classification image to then find the average pixel difference for the image. This process is repeated across the training set to find the most similar image(s). In this study the closest one, three and five (k = 1, 3, 5) to classify images will be used.</p><h3 id="support-vector-machines">Support Vector Machines</h3><p>Support Vector Machines (SVMs) for non-linear classification were developed in 1992 when Vladimir Vapnik introduced kernel methods to maximum-margin hyperplanes (Vladimir Vapnik, 1992). This kernel trick allows the mapping of training data to be translated into higher dimensions and for them to be calculated in a method which allows for a hyperplane to be implemented which divides the different categories in these new dimensions. These SVMs generally contain tens or hundreds of kernel tricks in high dimensionality data such as images. Support Vector Machines are used in this study to demonstrate the performance of a fair middle ground between symbolic and connectionist approaches to solving large datasets.</p><h2 id="results">Results</h2><p>The results of this study are judged on model performance on the two styles of datasets, lower or higher dimensionality and higher or lower training data respectively alongside their respective training and inference time.</p><h3 id="handwritten-digits">Handwritten Digits</h3><p>The MNIST handwritten digits showed that Symbolic methods can compete reliably with more data. The standard deviation of the accuracy of the models was 8.60e-3 and had a ±0.7% margin of error given a 95% confidence level.</p><p>Despite deep learning and transfer learning still coming across with the highest accuracy of the methods, one should consider the trade-off of using a Symbolic method in areas where algorithmic explainability is important. Notably, KNN still performed well within a competitive range for this dataset. <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/2021-06-17/handwritten_accuracy.png" alt="Handwritten Accuracies" /> <em>Fig 2. Model accuracies on the MNIST dataset</em></p><h3 id="nz-native-birds-1">NZ Native Birds</h3><p>Due to the lack of training data, the NZ Native Birds dataset highlights the strengths of having a transfer learning approach. The standard deviation of this dataset’s accuracies is 0.240, assuming a 95% confidence level the models had a ±44.4% margin of error.</p><p>As shown in Figure 3, Transfer Learning outperformed the rest of the models by more than 40%. Since machine learning is such a data driven practice it becomes important to have an already robust model before training to a such specific dataset. One must also look at the results with the understanding that no image augmentations or dimensionality reduction had taken place as it is outside the scope of this research. <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/2021-06-17/birds_accuracy.png" alt="Bird Accuracies" /> <em>Fig 3. Model accuracies on the Birds dataset</em></p><h3 id="summary">Summary</h3><p>Overall, the performance of Symbolic approaches on data that is of a high concentration and of a reasonable dimensionality remains relevant. Data science and machine learning practitioners should remain diligent and responsible by considering all approaches to solving problems rather than choosing models with many parameters especially in areas where users are greatly affected e.g., Healthcare or Government.</p><p>One major drawback of KNN however is its inference time. Generally, machine learning models are expected to have high training costs but can then be deployed onto low-cost infrastructure or directly onto low powered devices. Since, at inference time, KNN compares each training set image to the test image the inference time is directly proportional to the size of the training dataset. This causes issues on high-bandwidth data such as video streams. This effect is shown in Fig 6, training time is not shown due to KNN not requiring any changes to the training dataset or creating a prediction model off the data.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/2021-06-17/overall_accuracy.png" alt="Both Dataset Accuracies" /> <em>Fig 4. Model results comparatively across datasets</em> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/2021-06-17/training_time.png" alt="Training Times" /> <em>Fig 5. Model training time in seconds</em> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/2021-06-17/inference_time.png" alt="Inference Times" /> <em>Fig 5. Model inference time in milliseconds</em></p><h2 id="further-research">Further Research</h2><p>Artificial Intelligence is an ever-expanding field, while the popularity grows the importance of data ethics, privacy and algorithmic integrity increases with it. This research has attempted to outline the importance of these issues and to look at multiple approaches of different complexities and interpretability. However, in comparison to the wide range of techniques in the field of artificial intelligence the study is rather limited by the three models and the two datasets used. Further research may look at expanding the scope of algorithms and datasets, for example moving towards other types of data e.g., tabular, text or a combination of types. Additionally, there is clear evidence that concern over AI is growing and regulatory measures are starting to expand to limit the use of certain approaches, this suggests the importance of revisiting more traditional approaches to problems and further improving algorithms from a symbolic perspective.</p><p>Future research may also look at integrating both Symbolic and Connectionist in a Neuro-symbolic approach such as combining decision trees and neural networks such as the approach that Animesh Garg took towards creating generalizable autonomy for robotic applications (Garg, 2020). This has the potential to bring together the strong performance of deep learning with the clear desired outputs that symbolic methods provide (Garcez &amp; C. Lamb, 2020).</p><h2 id="references">References</h2><p>Allen, J. F. (1998, December 15). AI Growing Up: The changes and Opportunities. AI Magazine, pp. 13-23.</p><p>Brownlee, J. (2020, December 10). Machine Learning is Popular Right Now. Retrieved from Machine Learning Mastery: https://machinelearningmastery.com/machine-learning-is-popular/#:~:text=Machine%20learning%20is%20popular%20because,capability%20of%20machine%20learning%20methods.&amp;text=There%20is%20an%20abundance%20of%20data%20to%20learn%20from.</p><p>Brynce Goodman, S. F. (2016, August 31). European Union Regulations on Algorithmic Decision-Making and a “Right to Explanation”. AI Magazine, pp. 50-57. doi:10.1609/aimag.v38i3.2741</p><p>Bunz, M. (2019). The calculation of meaning: on the misunderstanding of new artificial intelligence as culture. Culture, Theory and Critique, 60, pp. 264-278. doi:10.1080/14735784.2019.1667255</p><p>Christian Berghoff, M. N. (2020, July 22). Vunerabilities of Connectionist AI Applications: Evaluation and Defense. Front. Big Data. doi:10.3389/fdata.2020.00023</p><p>Dean, J. (2019). The Deep Learning Revolution and Its Implications for Computer Architecture. CoRR, abs/1911.05289. Retrieved from http://arxiv.org/abs/1911.05289</p><p>Evenlyn Fix, J. H. (1951, February). Nonparametric Discrimination Consistency Properties. International Statistical Review / Revue Internationale de Statistique, 57, pp. 238-247. doi:https://doi.org/10.2307/1403797</p><p>Garcez, A. d., &amp; C. Lamb, L. (2020). Neurosymbolic AI: The 3rd Wave. arXiv(arXiv:2012.05876).</p><p>Garg, A. (2020, March 28). MIT 6.S191 (2020): Generalizable Autonomy for Robot Manipulation. Retrieved from Youtube: https://www.youtube.com/watch?v=8Kn4Gi8iSYQ&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=20&amp;ab_channel=AlexanderAminiAlexanderAmini</p><p>Hao Li, Z. X. (2017). Visualizing the Loss Landscape of Neural Nets. CoRR, abs/1712.09913. Retrieved from http://arxiv.org/abs/1712.09913 Hornik, K. (1991). Approximation Capabilities of Multilayer Feedforward Networks. Neural Networks, 4(0893-6080), pp. 251-257. doi:https://doi.org/10.1016/0893-6080(91)90009-T</p><p>Ian J. Goodfellow, J. S. (2015, March 20). Explaining and Harnessing Adversarial Examples. CoRR, abs/1412.6572.</p><p>Institute for Healthcare Policy &amp; Innovation. (2018, March 21). What happens when an algorithm cuts your health care.</p><p>Kaggle Inc. (2021). Kaggle: Your Machine Learning and Data Science Community. Retrieved from Kaggle: https://www.kaggle.com/</p><p>Kaiming He, X. Z. (2016). Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778. Retrieved from http://arxiv.org/abs/1512.03385</p><p>Seshapanpu, J. (2020, April 16). Vanishing and Exploding Gradients in Neural Networks. Retrieved from Medium: https://medium.com/@sjakki/vanishing-and-exploding-gradients-in-neural-networks-5b3ee108a568</p><p>Stanford Image Lab. (2020). ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Retrieved from ImageNet: https://www.image-net.org/challenges/LSVRC/#:~:text=The%20ImageNet%20Large%20Scale%20Visual,image%20classification%20at%20large%20scale.&amp;text=Another%20motivation%20is%20to%20measure,indexing%20for%20retrieval%20and%20annotation.</p><p>Tao, S. (2019, August 13). Deep Neural Network Ensembles. LOD.</p><p>Vladimir Vapnik, B. B. (1992, July). A Training Algorithm for Optimal Margin Classifiers. Proceedings of the fifth annual workshop on Computational learning theory, pp. 144–152. doi:10.1145/130385.130401 Yann LeCun, C. C. (n.d.). The MNIST Database. Retrieved from Lecun: http://yann.lecun.com/exdb/mnist/index.html</p><p>Yingqui Liu, S. M.-C. (2018, January). Trojaning Attack on Neural Networks. doi:10.14722/ndss.2018.23300</p><p>Yiwem Gui, C. Z. (2018). Sparse DNNs with Improved Adversarial Robustness. NeurIPS.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/research/'>Research</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/deeplearning/" class="post-tag no-text-decoration" >DeepLearning</a> <a href="/tags/fastai/" class="post-tag no-text-decoration" >FastAI</a> <a href="/tags/research/" class="post-tag no-text-decoration" >Research</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Performance of Symbolic & Connectionist AI in Image Recognition - Max van Dijck&url=/posts/Performance-of-Symbolic-&-Connectionist-AI-in-Image-Recognition/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Performance of Symbolic & Connectionist AI in Image Recognition - Max van Dijck&u=/posts/Performance-of-Symbolic-&-Connectionist-AI-in-Image-Recognition/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Performance of Symbolic & Connectionist AI in Image Recognition - Max van Dijck&url=/posts/Performance-of-Symbolic-&-Connectionist-AI-in-Image-Recognition/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/deeplearning/">DeepLearning</a> <a class="post-tag" href="/tags/fastai/">FastAI</a> <a class="post-tag" href="/tags/research/">Research</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Tricks-for-Training-a-State-of-the-art-Deep-Learning-model-for-Computer-Vision-in-Fast-AI/"><div class="card-body"> <span class="timeago small" > May 7, 2021 <i class="unloaded">2021-05-07T21:00:00+12:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Tricks for Training a State-of-the-art Deep Learning model for Computer Vision in Fast.AI</h3><div class="text-muted small"><p> There exists many ways to easily and effectively improve the accuracy of a neural network simply by making small changes to the data, training pipeline or how we run inference. This post aims to ou...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Tricks-for-Training-a-State-of-the-art-Deep-Learning-model-for-Computer-Vision-in-Fast-AI/" class="btn btn-outline-primary" prompt="Older"><p>Tricks for Training a State-of-the-art Deep Learning model for Computer Vision in Fast.AI</p></a> <span class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></span></div><div id="disqus" class="pt-2 pb-2"><p class="text-center text-muted small pb-5"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/assets/js/lib/jquery.disqusloader.min.js"></script> <script> const options = { scriptUrl: '//Max.disqus.com/embed.js', disqusConfig: function() { this.page.title = 'Performance of Symbolic & Connectionist AI in Image Recognition'; this.page.url = '/posts/Performance-of-Symbolic-&-Connectionist-AI-in-Image-Recognition/'; this.page.identifier = '/posts/Performance-of-Symbolic-&-Connectionist-AI-in-Image-Recognition/'; } }; $.disqusLoader('#disqus', options); </script></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#main > div.row:first-child > div:first-child img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/Max_vanDijck">Max van Dijck</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/deeplearning/">DeepLearning</a> <a class="post-tag" href="/tags/fastai/">FastAI</a> <a class="post-tag" href="/tags/research/">Research</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
